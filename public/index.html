
<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="lib/template.v2.js"></script>
  <script src="lib/d3.v7.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
</head>

<body>
  <!-- <distill-header></distill-header> -->
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "A journey into DataRealm",
    "description": "Although \" extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.",
    "published": "Aug 9, 2023",
    "authors": [
      {
        "author":"Lalith Sharan",
        "authorURL":"",
        "affiliations": [{"name": "AICM, Heidelberg", "url": "https://www.klinikum.uni-heidelberg.de/chirurgische-klinik-zentrum/herzchirurgie/forschung/ag-artificial-intelligence-in-cardiovascular-medicine"}]
      },
      {
        "author":"Georgii Kostiuchik",
        "authorURL":"",
        "affiliations": [{"name": "AICM, Heidelberg", "url": "https://www.klinikum.uni-heidelberg.de/chirurgische-klinik-zentrum/herzchirurgie/forschung/ag-artificial-intelligence-in-cardiovascular-medicine"}]
       },
      {
        "author":"Sandy Engelhardt",
        "authorURL":"",
        "affiliations": [{"name": "AICM, Heidelberg", "url": "https://www.klinikum.uni-heidelberg.de/chirurgische-klinik-zentrum/herzchirurgie/forschung/ag-artificial-intelligence-in-cardiovascular-medicine"}]
         }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <p>This is a description of this article and a submission to the MICCAI Educational Challenge</p>
  </d-title>
  <d-byline></d-byline>
  <d-article>
    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <p>It was on a warm and breezy Friday afternoon that Luca, having indulged in a heavy lunch, found himself gazing at a determined squirrel
that was perched on his window sill. The squirrel, clutching a stubborn nutshell with its tiny paws, executed an array of maneuvers with
intense focus. It began by turning the nut around, testing its angles and seams, while steadily biting and gnawing with determination.
Luca couldn't help draw a parallel to the time he had spent poring over lines of code, navigating the maze of his dataset with little success.
Memories came rushing of the hours spent babysitting his model, in pursuit of a reasonable loss curve. Despite his best efforts,
the model that had performed so well on the reported data inexplicably and repeatedly failed on his dataset. The model doesn't generalize,
_it just doesn't generalize_. At that very moment, a gentle breeze swept in and comforted him. Gradually, he sensed his thoughts slowing down,
his focus dwindling and his eyelids growing heavier...when an unexpected touch on his shoulder jolted him.

"Come with me. Now. There's no time to explain"
"Wh..What?"
    </p>

    <h3>Chapter 1: A failed effort</h3>
        <p>And so it was, that Luca now found himself in a strange and peculiar land. What he saw seemed both familiar and foreign to him, as he stared in disbelief
    at the boundless expanse that lay in front of him. Suddenly, Luca's thoughts turned to the unfinished work, his model that was probably training, the cluttered desk and the creaky chair that he
    left behind, all of which seemed like a world far far away from the one he was now standing in.
    He was overcome with confusion regarding the nature of his journey and the identity of the woman who had accompanied
    him here. As he glanced to his left in her direction, he had a feeling that she could perceive his confusion. As if in response, she proceeded to talk,
    'Greetings to the land of DataRealm, where all data lives. Here, information flows like rivers and mountains of code rise into the sky. This is a realm of creation and discovery, where the human spirit and the digital domain converge to form a connected whole.'</p>

      <!-- Chapter 1 -->
      <p> And so it was, that Luca now found himself in a strange and peculiar land. What he saw seemed both familiar and foreign to him, as he stared in disbelief
    at the boundless expanse that lay in front of him. Suddenly, Luca's thoughts turned to the unfinished work, his model that was probably training, the cluttered desk and the creaky chair that he
    left behind, all of which seemed like a world far far away from the one he was now standing in.
    He was overcome with confusion regarding the nature of his journey and the identity of the woman who had accompanied
    him here. As he glanced to his left in her direction, he had a feeling that she could perceive his confusion. As if in response, she proceeded to talk,
    'Greetings to the land of DataRealm, where all data lives. Here, information flows like rivers and mountains of code rise into the sky. This is a realm of creation and discovery, where the human spirit and the digital domain converge to form a connected whole.' </p>

      <p> As his eyes swept across the landscape, it slowly dawned upon him, as he faintly noticed gigantic crystalline structures in a distance that stood tall,
intricate with complex algorithms and databases. Interlinked nodes and circuits sprawled out like arteries, each a nexus of complex information, thoughts, and ideas.
It was as if the whole world around him was drowning in the very essence of information and understanding. Sensing his bewilderment, she continued, 'Deep within the heart of this land lies the _Archive of Wisdom_,
a treasure of all what humanity has thus far learnt from data. I see you have been in a lot of turmoil. Worry not, I am now going to take you there.'
Luca was just processing all that he had heard, wondering if this was all an elaborate joke. 'However my friend, we first need to pass through the three majestic spires that stand in front of us.'
</p>

<p> She held his hand and stepped toward the three tall structures that stood immediately before him, pulsating with energy, emitting soft and harmonious hums that echoed through the city behind it.
These towers stand in memoriam of all the data that have been misused, neglected, tortured, and sacrificed to fit a narrative. Luca, remember what they stand for, that you shall not repeat
the same mistakes. The first spire to your right, stands in the name of _Data Integrity_. It teaches us to always explore and understand your data before plunging into a task.
    Data has many insights that can be unearthed only through earnest poking and prying, and then lingering to listen to what it has to say. </p>

<p> 'How do I do this?' asked Luca feverishly. She smiled and proceeded, 'Your first step is to perform an exploratory data analysis. Here, you need to explore the distribution of your dataset, and
pause to think if it agrees with the underlying assumptions made about the data that you have observed. In more concrete terms, first understand the data distribution by plotting histograms, and looking for
any obvious outliers. Additionally, you can use pair plots to visualise the pair wise occurrences of features or the correlations between features. Further, dimensionality reduction methods like principal component analysis,
    and t-SNE visualisations help determine the salient features of your data. </p>

<!-- sample image of pair plot and feature distribution plot -->

<p> 'Luca, answer me this', she said, softly looking upward to the middle spire. _Crystal clear when uttered aloud, yet the crowd's notice I often shroud. What am I?_. After seeing Luca's slightly embarrassed and pondering look,
she continued '_Data Relevance_'. A lot of people take it for granted that defining a task on a dataset automatically implies the data is relevant to the task, but this may not be true in all cases. Let me explain. There is
a well-known story in our realm about the significant linear correlation discovered between the per-capita consumption of chocolate in a country and the Nobel laureates that it produces (r=0.791, P<0.0001).
Although this correlation is undeniable, you would do well to stop and think about possible confounders that may introduce spurious correlations into your model of the world.
You might need to look further and investigate the possible causal factors that may underlie both these observed effects. Let me give you another example Luca. During an investigation of a rule-based model to detect
risk of pneumonia, they found that asthma patients showed a lower risk of pneumonia. This is counter-intuitive but deeper investigation showed that the asthmatic patients who were directly admitted to the intensive care
    unit, received better care that resulted in a now decreased risk of pneumonia compared to the rest of the population. </p>


<!-- image of correlation between chocolates and Nobel Laureates -->

<p> And finally, remember, splitting your data can also end up introducing spurious correlations in your dataset. Remember the time when this work, which presented state-of-the-art performance that was better
than human radiologists in detecting pneumonia from chest X-rays. This work received flak in lieu of the data splitting that was used. Now I am not questioning the validity of this work, but rather pointing your attention
to the importance of explicitly outlining your data splits in the pipeline to the extent possible. It is not trivial, as we will
see later in our journey. I now urge you to pay your respects to the structure standing on the left in the name of _Data Security_. The data that is valuable to us can serve us only as long as we can protect
it, and not let it fall under the hands of bad actors. This means, ensuring anonymization and checking the data pipeline for potential
vulnerabilities. One must indeed to check the scope to which the data is exposed, and if necessary shield sensitive data from parts of your pipeline. Some may even turn to differential privacy or federated learning in
    this regard. </p>

<h3>Chapter 2: Shifting shores</h3>
<p> The two travellers then began their journey to Archive of wisdom at the heart of the realm. Luca's head was brimming
with all the information he had just received, while his companion, who now looked focused on her immediate surroundings,
turned to him and started in an anxious tone, 'Dear Luca, a little while ago I told you about data splitting and how it could introduce
spurious relations learnt from your dataset. This is a bigger issue, and we need to speak about it if you intend to proceed further into the realm.
It is our utmost responsibility while splitting data into train, test, and validation sets, or while splitting into cross-validation folds, that
you ensure there is no data leakage. Multitudes before you have failed to ensure this simple fact and have paid the price for it. There is no
place for these people in the heart of data realm. Splitting your data randomly may sometimes lead to same frames from a surgery, or from a patient
to fall under both the training and validation set or across different folds. If not ensured during the splitting process, this mode of failure
is very difficult to detect at later stages of the pipeline, and you may just be convinced that your model performs better. It is a good idea to use sklearn functions
    to ensure there is no data leakage while splitting your data. </p>

        <svg id="stratification" width="100%"></svg>
  <div id="controls">
    <button>Sampling</button>
    <button>Shuffled sampling</button>
    <button>Stratified sampling</button>
  </div>

<p> She continued, with an eye toward her next destination that lay straight ahead of where they were walking. Data leakage is much more
obvious but splitting can affect the performance of the data in more subtle ways. For example, if you have different subgroups
in your dataset, you need to ensure that each split or the fold of the data adequately represents the distribution of your whole dataset.
This is called stratified sampling, and can for example be done using the ```sklearn.model_selection.StratifiedShuffleSplit```.
Otherwise, this will skew your results. Keep in mind that the distribution or the shape of the different classes in your folds must essentially reflect
    that of your dataset. Similarly, if your data has hierarchies, they must also be reflected in your different splits. </p>



    <h3>Chapter 3: Splitting and Wizardry</h3>

    <p>Luca was taking in all this new information, wondering if he actually did these things in his model pipeline. They now reached a forest where
the purported data wizard resided. You need to pick the right data, that will lead you to the path home. There are multiple follies made
previously by those who have trodden this path. If you can avoid these, you will be able to meet the Data Wizard who will send you home. She then
proceeded to explain, 'Firstly, be aware of data leakage. You generally have the choice to split the data on multiple levels. For example,
this can be a patient-level or the sample level, or a surgery-level. You need to think what kind of split makes the most sense. For example,
if there are multiple surgical videos, it makes sense to split the frames in such a way that the surgeries do not mix. This can be ensured by
using for example the sklearn function to check for data leakage.'</p>

    <p>The second common mistake, is failing to ensure subgroups are adequately represented in your data splits. Always ensure you sample the data
in a stratified manner. This helps pick the data in such a way that there is enough representation of the subgroups
in each group. This is called stratified sampling. Additionally, if there are further hierarchies in your data,
your data split needs to reflect these hierarchies while sampling. Finally, it is quite common that you come across data classes that are very
imbalanced. This is especially true if you are trying to detect rare cases, such as rare diseases. In these cases you can turn to data augmentation
to offset this imbalance, that is by adding more data. Alternatively you can sample in a balanced manner and leave out some data.</p>

    <p>As Luca was ready to walk the path of the enchanted forest, he saw a really bright light that blinded him and heard a loud thud.
When he opened his eyes, he saw his desk and found himself fallen from his chair in his office. He quickly looks outside the window,
the squirrel was enjoying the magnificent treasures of its labour with a glint in its eye. </p>

  </d-article>

  <d-appendix>

    <h3>Contributions</h3>
    <p>Some text describing who did what.</p>
    <h3>Reviewers</h3>
    <p>Some text with links describing who reviewed the article.</p>

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

  <distill-footer></distill-footer>
  <script src="stratification.js" type="module"></script>

</body>
